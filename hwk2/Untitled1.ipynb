{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import struct\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from matplotlib.colors import colorConverter, ListedColormap # some plotting functions\n",
    "import itertools\n",
    "import collections\n",
    "plt.rcParams['figure.figsize'] = (4,4) # Make the figures a bit bigger\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the original binary MNIST data files in 0-255\n",
    "def read(dataset = \"training\", path = \".\"):\n",
    "    if dataset is \"training\":\n",
    "        fname_img = os.path.join(path, 'train-images-idx3-ubyte')\n",
    "        fname_lbl = os.path.join(path, 'train-labels-idx1-ubyte')\n",
    "    elif dataset is \"testing\":\n",
    "        fname_img = os.path.join(path, 't10k-images-idx3-ubyte')\n",
    "        fname_lbl = os.path.join(path, 't10k-labels-idx1-ubyte')\n",
    "    else:\n",
    "        raise ValueError( \"It needs to be between 'testing' and 'training'\")\n",
    "\n",
    "    with open(fname_lbl, 'rb') as flbl:\n",
    "        magic, num = struct.unpack(\">II\", flbl.read(8))\n",
    "        lbl = np.fromfile(flbl, dtype=np.int8)\n",
    "        # print(len(lbl))\n",
    "    with open(fname_img, 'rb') as fimg:\n",
    "        magic, num, rows, cols = struct.unpack(\">IIII\", fimg.read(16))\n",
    "        image = np.fromfile(fimg, dtype=np.uint8)      \n",
    "        image = image.reshape(len(lbl), rows, cols)\n",
    "\n",
    "    get_image = lambda idx: (lbl[idx], image[idx])\n",
    "\n",
    "    for i in range(len(lbl)):\n",
    "        yield get_image(i)\n",
    "\n",
    "training_data = list(read(dataset = \"training\",path = r'C:\\Users\\Han\\Desktop\\Box Sync\\CS 559\\hwk2'))\n",
    "testing_data = list(read(dataset = \"testing\",path = r'C:\\Users\\Han\\Desktop\\Box Sync\\CS 559\\hwk2'))\n",
    "training_label = np.zeros((len(training_data),1))\n",
    "training_desiredout =np.zeros((len(training_data),10))\n",
    "training_image = np.zeros((len(training_data),28*28))\n",
    "testing_label = np.zeros((len(testing_data),1))\n",
    "testing_desiredout = np.zeros((len(testing_data),10))\n",
    "testing_image = np.zeros((len(testing_data),28*28))\n",
    "\n",
    "# split the training and testing data to labels and images\n",
    "for i in range(len(training_data)):\n",
    "    temp = training_data[i]\n",
    "    training_label[i] = temp[0]\n",
    "    training_desiredout[i,temp[0]] = 1 \n",
    "    training_image[i,] = temp[1].reshape(1,28*28)\n",
    "#training_label = training_label.reshape((1,60000))\n",
    "for i in range(len(testing_data)):\n",
    "    temp = testing_data[i]\n",
    "    testing_label[i] = temp[0]\n",
    "    testing_desiredout[i,temp[0]] = 1 \n",
    "    testing_image[i,] = temp[1].reshape(1,28*28)\n",
    "#testing_label = testing_label.reshape((1,10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(training_image[0,:].reshape(28,28),cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = training_image\n",
    "X_test = testing_image\n",
    "T_train = training_label\n",
    "T_test = testing_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Dense(50, activation='tanh', input_shape=(784,)))\n",
    "#model.add(Dense(1, activation='sigmoid'))\n",
    "#model.add(Flatten())\n",
    "#model.add(Dense(128, activation='relu'))\n",
    "#model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?model.fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.fit(X_train, Y_train, \n",
    "          batch_size=1, epochs=10, verbose=2,  validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the non-linear functions used\n",
    "def logistic(z): \n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def logistic_deriv(y):  # Derivative of logistic function\n",
    "    return np.multiply(y, (1 - y))\n",
    "    \n",
    "def softmax(z): \n",
    "    return np.exp(z) / np.sum(np.exp(z), axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the layers used in this model\n",
    "class Layer(object):\n",
    "    \"\"\"Base class for the different layers.\n",
    "    Defines base methods and documentation of methods.\"\"\"\n",
    "    \n",
    "    def get_params_iter(self):\n",
    "        \"\"\"Return an iterator over the parameters (if any).\n",
    "        The iterator has the same order as get_params_grad.\n",
    "        The elements returned by the iterator are editable in-place.\"\"\"\n",
    "        return []\n",
    "    \n",
    "    def get_params_grad(self, X, output_grad):\n",
    "        \"\"\"Return a list of gradients over the parameters.\n",
    "        The list has the same order as the get_params_iter iterator.\n",
    "        X is the input.\n",
    "        output_grad is the gradient at the output of this layer.\n",
    "        \"\"\"\n",
    "        return []\n",
    "    \n",
    "    def get_output(self, X):\n",
    "        \"\"\"Perform the forward step linear transformation.\n",
    "        X is the input.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def get_input_grad(self, Y, output_grad=None, T=None):\n",
    "        \"\"\"Return the gradient at the inputs of this layer.\n",
    "        Y is the pre-computed output of this layer (not needed in this case).\n",
    "        output_grad is the gradient at the output of this layer \n",
    "         (gradient at input of next layer).\n",
    "        Output layer uses targets T to compute the gradient based on the \n",
    "         output error instead of output_grad\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LinearLayer(Layer):\n",
    "    \"\"\"The linear layer performs a linear transformation to its input.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_in, n_out):\n",
    "        \"\"\"Initialize hidden layer parameters.\n",
    "        n_in is the number of input variables.\n",
    "        n_out is the number of output variables.\"\"\"\n",
    "        self.W = np.random.randn(n_in, n_out) * 0.1\n",
    "        self.b = np.zeros(n_out)\n",
    "        \n",
    "    def get_params_iter(self):\n",
    "        \"\"\"Return an iterator over the parameters.\"\"\"\n",
    "        return itertools.chain(np.nditer(self.W, op_flags=['readwrite']),\n",
    "                               np.nditer(self.b, op_flags=['readwrite']))\n",
    "    \n",
    "    def get_output(self, X):\n",
    "        \"\"\"Perform the forward step linear transformation.\"\"\"\n",
    "        return X.dot(self.W) + self.b\n",
    "        \n",
    "    def get_params_grad(self, X, output_grad):\n",
    "        \"\"\"Return a list of gradients over the parameters.\"\"\"\n",
    "        JW = X.T.dot(output_grad)\n",
    "        Jb = np.sum(output_grad, axis=0)\n",
    "        return [g for g in itertools.chain(np.nditer(JW), np.nditer(Jb))]\n",
    "    \n",
    "    def get_input_grad(self, Y, output_grad):\n",
    "        \"\"\"Return the gradient at the inputs of this layer.\"\"\"\n",
    "        return output_grad.dot(self.W.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LogisticLayer(Layer):\n",
    "    \"\"\"The logistic layer applies the logistic function to its inputs.\"\"\"\n",
    "    \n",
    "    def get_output(self, X):\n",
    "        \"\"\"Perform the forward step transformation.\"\"\"\n",
    "        return logistic(X)\n",
    "    \n",
    "    def get_input_grad(self, Y, output_grad):\n",
    "        \"\"\"Return the gradient at the inputs of this layer.\"\"\"\n",
    "        return np.multiply(logistic_deriv(Y), output_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SoftmaxOutputLayer(Layer):\n",
    "    \"\"\"The softmax output layer computes the classification propabilities at the output.\"\"\"\n",
    "    \n",
    "    def get_output(self, X):\n",
    "        \"\"\"Perform the forward step transformation.\"\"\"\n",
    "        return softmax(X)\n",
    "    \n",
    "    def get_input_grad(self, Y, T):\n",
    "        \"\"\"Return the gradient at the inputs of this layer.\"\"\"\n",
    "        return (Y - T) / Y.shape[0]\n",
    "    \n",
    "    def get_cost(self, Y, T):\n",
    "        \"\"\"Return the cost at the output of this output layer.\"\"\"\n",
    "        return - np.multiply(T, np.log(Y)).sum() / Y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a sample model to be trained on the data\n",
    "hidden_neurons_1 = 200  # Number of neurons in the first hidden-layer\n",
    "hidden_neurons_2 = 20  # Number of neurons in the second hidden-layer\n",
    "# Create the model\n",
    "layers = [] # Define a list of layers\n",
    "# Add first hidden layer\n",
    "layers.append(LinearLayer(X_train.shape[1], hidden_neurons_1))\n",
    "layers.append(LogisticLayer())\n",
    "# Add second hidden layer\n",
    "layers.append(LinearLayer(hidden_neurons_1, hidden_neurons_2))\n",
    "layers.append(LogisticLayer())\n",
    "# Add output layer\n",
    "layers.append(LinearLayer(hidden_neurons_2, T_train.shape[1]))\n",
    "layers.append(SoftmaxOutputLayer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the forward propagation step as a method.\n",
    "def forward_step(input_samples, layers):\n",
    "    \"\"\"\n",
    "    Compute and return the forward activation of each layer in layers.\n",
    "    Input:\n",
    "        input_samples: A matrix of input samples (each row is an input vector)\n",
    "        layers: A list of Layers\n",
    "    Output:\n",
    "        A list of activations where the activation at each index i+1 corresponds to\n",
    "        the activation of layer i in layers. activations[0] contains the input samples.  \n",
    "    \"\"\"\n",
    "    activations = [input_samples] # List of layer activations\n",
    "    # Compute the forward activations for each layer starting from the first\n",
    "    X = input_samples\n",
    "    for layer in layers:\n",
    "        Y = layer.get_output(X)  # Get the output of the current layer\n",
    "        activations.append(Y)  # Store the output for future processing\n",
    "        X = activations[-1]  # Set the current input as the activations of the previous layer\n",
    "    return activations  # Return the activations of each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the backward propagation step as a method\n",
    "def backward_step(activations, targets, layers):\n",
    "    \"\"\"\n",
    "    Perform the backpropagation step over all the layers and return the parameter gradients.\n",
    "    Input:\n",
    "        activations: A list of forward step activations where the activation at \n",
    "            each index i+1 corresponds to the activation of layer i in layers. \n",
    "            activations[0] contains the input samples. \n",
    "        targets: The output targets of the output layer.\n",
    "        layers: A list of Layers corresponding that generated the outputs in activations.\n",
    "    Output:\n",
    "        A list of parameter gradients where the gradients at each index corresponds to\n",
    "        the parameters gradients of the layer at the same index in layers. \n",
    "    \"\"\"\n",
    "    param_grads = collections.deque()  # List of parameter gradients for each layer\n",
    "    output_grad = None  # The error gradient at the output of the current layer\n",
    "    # Propagate the error backwards through all the layers.\n",
    "    #  Use reversed to iterate backwards over the list of layers.\n",
    "    for layer in reversed(layers):   \n",
    "        Y = activations.pop()  # Get the activations of the last layer on the stack\n",
    "        # Compute the error at the output layer.\n",
    "        # The output layer error is calculated different then hidden layer error.\n",
    "        if output_grad is None:\n",
    "            input_grad = layer.get_input_grad(Y, targets)\n",
    "        else:  # output_grad is not None (layer is not output layer)\n",
    "            input_grad = layer.get_input_grad(Y, output_grad)\n",
    "        # Get the input of this layer (activations of the previous layer)\n",
    "        X = activations[-1]\n",
    "        # Compute the layer parameter gradients used to update the parameters\n",
    "        grads = layer.get_params_grad(X, output_grad)\n",
    "        param_grads.appendleft(grads)\n",
    "        # Compute gradient at output of previous layer (input of current layer):\n",
    "        output_grad = input_grad\n",
    "    return list(param_grads)  # Return the parameter gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Numerical gradient of 0.000000 is not close to the backpropagation gradient of 0.000015!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-f0e5ba9a4513>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;31m# Raise error if the numerical grade is not close to the backprop gradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad_num\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_backprop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Numerical gradient of {:.6f} is not close to the backpropagation gradient of {:.6f}!'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad_backprop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'No gradient errors found'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Numerical gradient of 0.000000 is not close to the backpropagation gradient of 0.000015!"
     ]
    }
   ],
   "source": [
    "# Perform gradient checking\n",
    "nb_samples_gradientcheck = 10 # Test the gradients on a subset of the data\n",
    "X_temp = X_train[0:nb_samples_gradientcheck,:]\n",
    "T_temp = T_train[0:nb_samples_gradientcheck,:]\n",
    "# Get the parameter gradients with backpropagation\n",
    "activations = forward_step(X_temp, layers)\n",
    "param_grads = backward_step(activations, T_temp, layers)\n",
    "\n",
    "# Set the small change to compute the numerical gradient\n",
    "eps = 0.0001\n",
    "# Compute the numerical gradients of the parameters in all layers.\n",
    "for idx in range(len(layers)):\n",
    "    layer = layers[idx]\n",
    "    layer_backprop_grads = param_grads[idx]\n",
    "    # Compute the numerical gradient for each parameter in the layer\n",
    "    for p_idx, param in enumerate(layer.get_params_iter()):\n",
    "        grad_backprop = layer_backprop_grads[p_idx]\n",
    "        # + eps\n",
    "        param += eps\n",
    "        plus_cost = layers[-1].get_cost(forward_step(X_temp, layers)[-1], T_temp)\n",
    "        # - eps\n",
    "        param -= 2 * eps\n",
    "        min_cost = layers[-1].get_cost(forward_step(X_temp, layers)[-1], T_temp)\n",
    "        # reset param value\n",
    "        param += eps\n",
    "        # calculate numerical gradient\n",
    "        grad_num = (plus_cost - min_cost)/(2*eps)\n",
    "        # Raise error if the numerical grade is not close to the backprop gradient\n",
    "        if not np.isclose(grad_num, grad_backprop):\n",
    "            raise ValueError('Numerical gradient of {:.6f} is not close to the backpropagation gradient of {:.6f}!'.format(float(grad_num), float(grad_backprop)))\n",
    "print('No gradient errors found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
